{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "data = pd.read_csv(\"../../data/sensitivity_data/sensitivity_dataset.csv\")\n",
    "data = data[[\"Filename\",\"Date\",\"Sensitivity\",\"Document\"]]\n",
    "\n",
    "# Train / Test split\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(data['Document'],data['Sensitivity'],test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_x).batch(128)\n",
    "vectoriser.adapt(text_ds)\n",
    "\n",
    "voc = vectoriser.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "glove_path = \"..\\deep_learning\\glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(glove_path,encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs,\"f\",sep= \" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens,embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(num_tokens,embedding_dim,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         2000200   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 512)         1255424   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 1)           513       \n",
      "=================================================================\n",
      "Total params: 3,256,137\n",
      "Trainable params: 1,255,937\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(embedded_sequences)\n",
    "dense = tf.keras.layers.Dense(1)(lstm)\n",
    "glove_model = tf.keras.Model(int_sequences_input, dense)\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, None, 1)           3256137   \n",
      "=================================================================\n",
      "Total params: 3,256,137\n",
      "Trainable params: 1,255,937\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "string_input = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectoriser(string_input)\n",
    "preds = glove_model(x)\n",
    "end_to_end_model = tf.keras.Model(string_input, preds)\n",
    "\n",
    "end_to_end_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "glove_model.compile(optimizer=optimiser,loss='binary_crossentropy',metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "95/95 [==============================] - 95s 945ms/step - loss: 2.0692 - binary_accuracy: 0.8658\n",
      "Epoch 2/5\n",
      "95/95 [==============================] - 96s 1s/step - loss: 2.0691 - binary_accuracy: 0.8657\n",
      "Epoch 3/5\n",
      "95/95 [==============================] - 87s 914ms/step - loss: 2.0675 - binary_accuracy: 0.8657\n",
      "Epoch 4/5\n",
      "95/95 [==============================] - 95s 1s/step - loss: 2.0205 - binary_accuracy: 0.8499\n",
      "Epoch 5/5\n",
      "95/95 [==============================] - 93s 983ms/step - loss: 2.0451 - binary_accuracy: 0.8653\n"
     ]
    }
   ],
   "source": [
    "x_train = vectoriser(np.array([[s] for s in train_x])).numpy()\n",
    "x_val = vectoriser(np.array([[s] for s in test_x])).numpy()\n",
    "\n",
    "y_train = np.array(train_y)\n",
    "y_val = np.array(test_y)\n",
    "\n",
    "history = glove_model.fit(x=x_train,y=y_train,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = glove_model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22040/2053277288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predicted sentiment : positive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'precicted sentiment : negative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,700)\n",
    "if predictions[n] > 0.5:\n",
    "  print('predicted sentiment : positive')\n",
    "else: \n",
    "  print('precicted sentiment : negative')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2292f7dadeec0b36dafabb7d1d8dd7b9b8b8f0665c515bec64e67bf9650aaf0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
