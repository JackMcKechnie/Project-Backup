{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense \n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv(\"../../../data/side_information.csv\",encoding='unicode_escape')\n",
    "checkpoint_path = \"C:/Users/jack-/Documents/University/Project/src/deep_learning/bert_tests/checkpoints\"\n",
    "feature_names = ['Sentence',\n",
    "                 'Length in Words', 'Length in Characters', 'F-score', 'I-score',\n",
    "                 'Lexical Density','FK Reading Ease', 'FOG Scale', 'SMOG Index', 'ARI',\n",
    "                 'CL Index', 'LW Formula', 'DC Score', 'Readability Consensus',\n",
    "                 'Spache Formula']\n",
    "\n",
    "samples = data[feature_names]\n",
    "labels = data[\"Formality\"]\n",
    "train_samples, test_samples, train_labels,test_labels = train_test_split(samples, labels, test_size=0.2,random_state=5)\n",
    "\n",
    "bert_train_samples = np.array(train_samples[\"Sentence\"])\n",
    "bert_test_samples = np.array(test_samples[\"Sentence\"])\n",
    "side_train_samples = np.array(train_samples[feature_names[1:]])\n",
    "side_test_samples = np.array(test_samples[feature_names[1:]])\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "test_samples = np.array(test_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "\n",
    "# Attention layer\n",
    "class peel_the_layer(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self,units=1):    \n",
    "        ##Nothing special to be done here\n",
    "        super(peel_the_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        ##Define the shape of the weights and bias in this layer\n",
    "        ##This is a 1 unit layer. \n",
    "        units=1\n",
    "        ##last index of the input_shape is the number of dimensions of the prev\n",
    "        ##RNN layer. last but 1 index is the num of timesteps\n",
    "        self.w=self.add_weight(name=\"att_weights\", shape=(input_shape[-1], units), initializer=\"normal\") #name property is useful for avoiding RuntimeError: Unable to create link.\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[-2], units), initializer=\"zeros\")\n",
    "        super(peel_the_layer,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        ##x is the input tensor..each word that needs to be attended to\n",
    "        ##Below is the main processing done during training\n",
    "        ##K is the Keras Backend import\n",
    "        e = K.tanh(K.dot(x,self.w)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "\n",
    "        ##return the ouputs. 'a' is the set of attention weights\n",
    "        ##the second variable is the 'attention adjusted o/p state' or context\n",
    "        return a, K.sum(output, axis=1)\n",
    "\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "\n",
    "\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliser = tf.keras.layers.Normalization()\n",
    "normaliser.adapt(side_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NORMALISED SIDE INFORMATION MODEL --\n",
    "\n",
    "# Bert model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "normalised_bert = tf.keras.Model(text_input, lstm)\n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(14))\n",
    "normalised = normaliser(side_input)\n",
    "reshaped = tf.reshape(normalised,[-1, 1, 14])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "normalised_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Combine models and predict\n",
    "combined = tf.keras.layers.concatenate([normalised_bert.output, normalised_side.output],axis=1)\n",
    "a, context = peel_the_layer()(combined)\n",
    "dense = tf.keras.layers.Dense(1)(context)\n",
    "normalised_model = tf.keras.Model(inputs=[normalised_bert.input, normalised_side.input], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PRETRAINED SIDE MODEL -- \n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(14),name=\"Side Information\")\n",
    "reshaped = tf.reshape(side_input,[-1, 1, 14])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "pretrained_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Pre-train side model\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta=0.01)\n",
    "pretrained_side.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "pretrained_side.fit(x=side_train_samples,y=train_labels,batch_size=5,epochs=100,verbose=0,callbacks=[callback])\n",
    "\n",
    "\n",
    "# Bert using pretrained side model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "side_input = tf.keras.layers.Input(shape=(14),)\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "a, context = peel_the_layer()(lstm)\n",
    "\n",
    "trained_side_input = pretrained_side(side_input)\n",
    "trained_side_input = tf.reshape(trained_side_input,[-1, 512])\n",
    "\n",
    "concat = tf.keras.layers.concatenate([context, trained_side_input]) \n",
    "\n",
    "\n",
    "dense = tf.keras.layers.Dense(1)(concat)\n",
    "\n",
    "pretrained_model = tf.keras.Model([text_input,side_input],dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- AVERAGE LAYER INSTEAD OF CONCATENATION --\n",
    "\n",
    "# Bert model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "average_bert = tf.keras.Model(text_input, lstm)\n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(14))\n",
    "reshaped = tf.reshape(side_input,[-1, 1, 14])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "average_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Combine models and predict\n",
    "combined = tf.keras.layers.average([average_bert.output, average_side.output])\n",
    "a, context = peel_the_layer()(combined)\n",
    "dense = tf.keras.layers.Dense(1)(context)\n",
    "average_model = tf.keras.Model(inputs=[average_bert.input, average_side.input], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "176/176 - 3058s - loss: 2.4430 - mean_absolute_error: 1.0333 - mean_absolute_percentage_error: 27.8895\n",
      "Epoch 2/8\n",
      "176/176 - 2876s - loss: 0.4441 - mean_absolute_error: 0.5321 - mean_absolute_percentage_error: 14.8510\n",
      "Epoch 3/8\n",
      "176/176 - 2871s - loss: 0.2997 - mean_absolute_error: 0.4348 - mean_absolute_percentage_error: 12.0294\n",
      "Epoch 4/8\n",
      "176/176 - 2873s - loss: 0.1921 - mean_absolute_error: 0.3419 - mean_absolute_percentage_error: 9.3433\n",
      "Epoch 5/8\n",
      "176/176 - 2871s - loss: 0.1227 - mean_absolute_error: 0.2715 - mean_absolute_percentage_error: 7.3048\n",
      "Epoch 6/8\n",
      "176/176 - 2874s - loss: 0.0807 - mean_absolute_error: 0.2209 - mean_absolute_percentage_error: 5.9655\n",
      "Epoch 7/8\n",
      "176/176 - 2869s - loss: 0.0613 - mean_absolute_error: 0.1935 - mean_absolute_percentage_error: 5.3141\n",
      "Epoch 8/8\n",
      "176/176 - 2865s - loss: 0.0466 - mean_absolute_error: 0.1677 - mean_absolute_percentage_error: 4.6752\n",
      "44/44 [==============================] - 233s 5s/step - loss: 0.5397 - mean_absolute_error: 0.5855 - mean_absolute_percentage_error: 16.2758\n",
      "Completed 8 Epochs\n",
      "[0.5396922826766968, 0.5855347514152527, 16.275794982910156]\n",
      "Epoch 1/20\n",
      "176/176 - 2869s - loss: 2.4439 - mean_absolute_error: 1.0538 - mean_absolute_percentage_error: 28.6599\n",
      "Epoch 2/20\n",
      "176/176 - 2859s - loss: 0.4243 - mean_absolute_error: 0.5199 - mean_absolute_percentage_error: 14.5227\n",
      "Epoch 3/20\n",
      "176/176 - 2858s - loss: 0.2543 - mean_absolute_error: 0.3965 - mean_absolute_percentage_error: 10.8536\n",
      "Epoch 4/20\n",
      "176/176 - 2874s - loss: 0.1382 - mean_absolute_error: 0.2920 - mean_absolute_percentage_error: 7.8476\n",
      "Epoch 5/20\n",
      "176/176 - 2858s - loss: 0.0902 - mean_absolute_error: 0.2354 - mean_absolute_percentage_error: 6.3828\n",
      "Epoch 6/20\n",
      "176/176 - 2848s - loss: 0.0626 - mean_absolute_error: 0.1945 - mean_absolute_percentage_error: 5.3321\n",
      "Epoch 7/20\n",
      "176/176 - 2854s - loss: 0.0455 - mean_absolute_error: 0.1663 - mean_absolute_percentage_error: 4.6129\n",
      "Epoch 8/20\n",
      "176/176 - 2858s - loss: 0.0321 - mean_absolute_error: 0.1384 - mean_absolute_percentage_error: 3.7994\n",
      "Epoch 9/20\n",
      "176/176 - 2857s - loss: 0.0324 - mean_absolute_error: 0.1401 - mean_absolute_percentage_error: 3.8682\n",
      "Epoch 10/20\n",
      "176/176 - 3189s - loss: 0.0308 - mean_absolute_error: 0.1375 - mean_absolute_percentage_error: 3.8066\n",
      "Epoch 11/20\n",
      "176/176 - 3711s - loss: 0.0262 - mean_absolute_error: 0.1267 - mean_absolute_percentage_error: 3.5068\n",
      "44/44 [==============================] - 269s 6s/step - loss: 0.5098 - mean_absolute_error: 0.5713 - mean_absolute_percentage_error: 16.0149\n",
      "Completed 20 Epochs\n",
      "[0.5097756385803223, 0.5713443160057068, 16.014867782592773]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E8 B32</th>\n",
       "      <td>0.509776</td>\n",
       "      <td>0.571344</td>\n",
       "      <td>16.014868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1          2\n",
       "E8 B32  0.509776  0.571344  16.014868"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- NORMALISED TRAINING AND TESTING --\n",
    "normalised_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta=0.01)\n",
    "\n",
    "normalised_model.save_weights('normalised.h5')\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_model.load_weights(\"normalised.h5\")\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SAVE NORMALISED RESULTS --\n",
    "normalised_results = normalised_results.T\n",
    "normalised_results.to_csv(\"./lstm_side_information_tests/normalised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "176/176 - 3437s - loss: 2.8942 - mean_absolute_error: 1.2067 - mean_absolute_percentage_error: 33.1861\n",
      "Epoch 2/8\n",
      "176/176 - 3526s - loss: 0.5185 - mean_absolute_error: 0.5779 - mean_absolute_percentage_error: 16.3963\n",
      "Epoch 3/8\n",
      "176/176 - 3188s - loss: 0.3432 - mean_absolute_error: 0.4655 - mean_absolute_percentage_error: 13.0815\n",
      "Epoch 4/8\n",
      "176/176 - 3295s - loss: 0.2184 - mean_absolute_error: 0.3653 - mean_absolute_percentage_error: 10.1201\n",
      "Epoch 5/8\n",
      "176/176 - 3426s - loss: 0.1294 - mean_absolute_error: 0.2802 - mean_absolute_percentage_error: 7.7037\n",
      "Epoch 6/8\n",
      "176/176 - 3647s - loss: 0.0773 - mean_absolute_error: 0.2159 - mean_absolute_percentage_error: 5.9346\n",
      "Epoch 7/8\n",
      "176/176 - 3508s - loss: 0.0522 - mean_absolute_error: 0.1767 - mean_absolute_percentage_error: 4.8852\n",
      "Epoch 8/8\n",
      "176/176 - 3089s - loss: 0.0423 - mean_absolute_error: 0.1584 - mean_absolute_percentage_error: 4.3352\n",
      "44/44 [==============================] - 236s 5s/step - loss: 0.5334 - mean_absolute_error: 0.5754 - mean_absolute_percentage_error: 16.1799\n",
      "Completed 8 Epochs\n",
      "[0.5333935022354126, 0.5753917694091797, 16.179872512817383]\n",
      "Epoch 1/20\n",
      "176/176 - 2863s - loss: 3.2253 - mean_absolute_error: 1.3384 - mean_absolute_percentage_error: 37.3714\n",
      "Epoch 2/20\n",
      "176/176 - 3081s - loss: 0.9018 - mean_absolute_error: 0.7698 - mean_absolute_percentage_error: 22.6271\n",
      "Epoch 3/20\n",
      "176/176 - 2878s - loss: 0.7470 - mean_absolute_error: 0.6952 - mean_absolute_percentage_error: 19.9636\n",
      "Epoch 4/20\n",
      "176/176 - 3107s - loss: 0.7378 - mean_absolute_error: 0.6900 - mean_absolute_percentage_error: 19.8558\n",
      "Epoch 5/20\n",
      "176/176 - 2889s - loss: 0.7293 - mean_absolute_error: 0.6866 - mean_absolute_percentage_error: 19.6778\n",
      "Epoch 6/20\n",
      "176/176 - 2849s - loss: 0.7283 - mean_absolute_error: 0.6856 - mean_absolute_percentage_error: 19.6105\n",
      "Epoch 7/20\n",
      "176/176 - 2953s - loss: 0.7238 - mean_absolute_error: 0.6827 - mean_absolute_percentage_error: 19.5006\n",
      "Epoch 8/20\n",
      "176/176 - 2759s - loss: 0.7232 - mean_absolute_error: 0.6818 - mean_absolute_percentage_error: 19.5062\n",
      "44/44 [==============================] - 223s 5s/step - loss: 0.7035 - mean_absolute_error: 0.6674 - mean_absolute_percentage_error: 19.6972\n",
      "Completed 20 Epochs\n",
      "[0.7034713625907898, 0.6674169898033142, 19.69721221923828]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E8 B32</th>\n",
       "      <td>0.703471</td>\n",
       "      <td>0.667417</td>\n",
       "      <td>19.697212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1          2\n",
       "E8 B32  0.703471  0.667417  19.697212"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --PRETRAINED TRAINING AND TESTING --\n",
    "pretrained_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta=0.01)\n",
    "\n",
    "pretrained_model.save_weights('pretrained.h5')\n",
    "pretrained_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "pretrained_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = pretrained_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "pretrained_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "pretrained_model.load_weights('pretrained.h5')\n",
    "pretrained_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "pretrained_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = pretrained_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "pretrained_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "pretrained_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/lstm_side_information_tests/pretrained.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29568/2006224017.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# -- SAVE PRETRAINED RESULTS --\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpretrained_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrained_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpretrained_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/lstm_side_information_tests/pretrained.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/lstm_side_information_tests/pretrained.csv'"
     ]
    }
   ],
   "source": [
    "# -- SAVE PRETRAINED RESULTS --\n",
    "pretrained_results = pretrained_results.T\n",
    "pretrained_results.to_csv(\"/lstm_side_information_tests/pretrained.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "176/176 - 2931s - loss: 2.7868 - mean_absolute_error: 1.1611 - mean_absolute_percentage_error: 31.2592\n",
      "Epoch 2/8\n",
      "176/176 - 2870s - loss: 0.7489 - mean_absolute_error: 0.6970 - mean_absolute_percentage_error: 20.0374\n",
      "Epoch 3/8\n",
      "176/176 - 2869s - loss: 0.7428 - mean_absolute_error: 0.6928 - mean_absolute_percentage_error: 19.8413\n",
      "Epoch 4/8\n",
      "176/176 - 2914s - loss: 0.7306 - mean_absolute_error: 0.6866 - mean_absolute_percentage_error: 19.6622\n",
      "Epoch 5/8\n",
      "176/176 - 2870s - loss: 0.7253 - mean_absolute_error: 0.6853 - mean_absolute_percentage_error: 19.5648\n",
      "Epoch 6/8\n",
      "176/176 - 2877s - loss: 0.7214 - mean_absolute_error: 0.6822 - mean_absolute_percentage_error: 19.5078\n",
      "Epoch 7/8\n",
      "176/176 - 2875s - loss: 0.7207 - mean_absolute_error: 0.6826 - mean_absolute_percentage_error: 19.4445\n",
      "44/44 [==============================] - 233s 5s/step - loss: 0.7555 - mean_absolute_error: 0.6876 - mean_absolute_percentage_error: 20.3328\n",
      "Completed 8 Epochs\n",
      "[0.755474328994751, 0.6876088976860046, 20.332841873168945]\n",
      "Epoch 1/20\n",
      "176/176 - 2919s - loss: 2.6856 - mean_absolute_error: 1.1496 - mean_absolute_percentage_error: 31.1790\n",
      "Epoch 2/20\n",
      "176/176 - 2906s - loss: 0.8469 - mean_absolute_error: 0.7143 - mean_absolute_percentage_error: 20.4357\n",
      "Epoch 3/20\n",
      "176/176 - 2900s - loss: 0.7386 - mean_absolute_error: 0.6911 - mean_absolute_percentage_error: 19.8010\n",
      "Epoch 4/20\n",
      "176/176 - 2905s - loss: 0.7340 - mean_absolute_error: 0.6891 - mean_absolute_percentage_error: 19.7402\n",
      "Epoch 5/20\n",
      "176/176 - 2909s - loss: 0.7252 - mean_absolute_error: 0.6832 - mean_absolute_percentage_error: 19.5164\n",
      "Epoch 6/20\n",
      "176/176 - 2927s - loss: 0.7298 - mean_absolute_error: 0.6867 - mean_absolute_percentage_error: 19.5457\n",
      "Epoch 7/20\n",
      "176/176 - 2941s - loss: 0.7185 - mean_absolute_error: 0.6819 - mean_absolute_percentage_error: 19.4647\n",
      "Epoch 8/20\n",
      "176/176 - 2794s - loss: 0.7172 - mean_absolute_error: 0.6788 - mean_absolute_percentage_error: 19.3324\n",
      "44/44 [==============================] - 224s 5s/step - loss: 0.6935 - mean_absolute_error: 0.6698 - mean_absolute_percentage_error: 18.7730\n",
      "Completed 20 Epochs\n",
      "[0.6935076713562012, 0.6698284149169922, 18.773012161254883]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E8 B32</th>\n",
       "      <td>0.693508</td>\n",
       "      <td>0.669828</td>\n",
       "      <td>18.773012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1          2\n",
       "E8 B32  0.693508  0.669828  18.773012"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --AVERAGE LAYER TRAINING AND TESTING --\n",
    "average_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta=0.01)\n",
    "\n",
    "\n",
    "average_model.save_weights('average.h5')\n",
    "average_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "average_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = average_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "average_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "average_model.load_weights('average.h5')\n",
    "average_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "average_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = average_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "average_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "average_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SAVE AVERAGE RESULTS --\n",
    "average_results = average_results.T\n",
    "average_results.to_csv(\"./lstm_side_information_tests/average.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2292f7dadeec0b36dafabb7d1d8dd7b9b8b8f0665c515bec64e67bf9650aaf0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
