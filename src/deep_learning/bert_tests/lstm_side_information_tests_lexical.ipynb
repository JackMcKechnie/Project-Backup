{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense \n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv(\"../../../data/side_information.csv\",encoding='unicode_escape')\n",
    "checkpoint_path = \"C:/Users/jack-/Documents/University/Project/src/deep_learning/bert_tests/checkpoints\"\n",
    "feature_names = ['Sentence','Length in Words','Length in Characters','F-score','I-score','Lexical Density','Adjective Density','Spellcheck Percentage','Syllable Ratio','Latinate vs Germanic']\n",
    "\n",
    "\n",
    "samples = data[feature_names]\n",
    "labels = data[\"Formality\"]\n",
    "train_samples, test_samples, train_labels,test_labels = train_test_split(samples, labels, test_size=0.2,random_state=5)\n",
    "\n",
    "bert_train_samples = np.array(train_samples[\"Sentence\"])\n",
    "bert_test_samples = np.array(test_samples[\"Sentence\"])\n",
    "side_train_samples = np.array(train_samples[feature_names[1:]])\n",
    "side_test_samples = np.array(test_samples[feature_names[1:]])\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "test_samples = np.array(test_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "\n",
    "# Attention layer\n",
    "class peel_the_layer(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self,units=1):    \n",
    "        ##Nothing special to be done here\n",
    "        super(peel_the_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        ##Define the shape of the weights and bias in this layer\n",
    "        ##This is a 1 unit layer. \n",
    "        units=1\n",
    "        ##last index of the input_shape is the number of dimensions of the prev\n",
    "        ##RNN layer. last but 1 index is the num of timesteps\n",
    "        self.w=self.add_weight(name=\"att_weights\", shape=(input_shape[-1], units), initializer=\"normal\") #name property is useful for avoiding RuntimeError: Unable to create link.\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[-2], units), initializer=\"zeros\")\n",
    "        super(peel_the_layer,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        ##x is the input tensor..each word that needs to be attended to\n",
    "        ##Below is the main processing done during training\n",
    "        ##K is the Keras Backend import\n",
    "        e = K.tanh(K.dot(x,self.w)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "\n",
    "        ##return the ouputs. 'a' is the set of attention weights\n",
    "        ##the second variable is the 'attention adjusted o/p state' or context\n",
    "        return a, K.sum(output, axis=1)\n",
    "\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "\n",
    "\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliser = tf.keras.layers.Normalization()\n",
    "normaliser.adapt(side_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NORMALISED SIDE INFORMATION MODEL --\n",
    "\n",
    "# Bert model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "normalised_bert = tf.keras.Model(text_input, lstm)\n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(9))\n",
    "normalised = normaliser(side_input)\n",
    "reshaped = tf.reshape(normalised,[-1, 1, 9])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "normalised_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Combine models and predict\n",
    "combined = tf.keras.layers.concatenate([normalised_bert.output, normalised_side.output],axis=1)\n",
    "a, context = peel_the_layer()(combined)\n",
    "dense = tf.keras.layers.Dense(1)(context)\n",
    "normalised_model = tf.keras.Model(inputs=[normalised_bert.input, normalised_side.input], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "176/176 - 2844s - loss: 2.5832 - mean_absolute_error: 1.0802 - mean_absolute_percentage_error: 28.9913\n",
      "Epoch 2/8\n",
      "176/176 - 2853s - loss: 0.4405 - mean_absolute_error: 0.5295 - mean_absolute_percentage_error: 14.7484\n",
      "Epoch 3/8\n",
      "176/176 - 2851s - loss: 0.2712 - mean_absolute_error: 0.4114 - mean_absolute_percentage_error: 11.2731\n",
      "Epoch 4/8\n",
      "176/176 - 2850s - loss: 0.1581 - mean_absolute_error: 0.3108 - mean_absolute_percentage_error: 8.5281\n",
      "Epoch 5/8\n",
      "176/176 - 2849s - loss: 0.0915 - mean_absolute_error: 0.2352 - mean_absolute_percentage_error: 6.4103\n",
      "Epoch 6/8\n",
      "176/176 - 2849s - loss: 0.0643 - mean_absolute_error: 0.1987 - mean_absolute_percentage_error: 5.4255\n",
      "Epoch 7/8\n",
      "176/176 - 2781s - loss: 0.0469 - mean_absolute_error: 0.1702 - mean_absolute_percentage_error: 4.6509\n",
      "Epoch 8/8\n",
      "176/176 - 2658s - loss: 0.0435 - mean_absolute_error: 0.1617 - mean_absolute_percentage_error: 4.4712\n",
      "44/44 [==============================] - 217s 5s/step - loss: 0.5031 - mean_absolute_error: 0.5669 - mean_absolute_percentage_error: 16.0755\n",
      "Completed 8 Epochs\n",
      "[0.5031061768531799, 0.5668861269950867, 16.075475692749023]\n",
      "Epoch 1/20\n",
      "176/176 - 2681s - loss: 2.5892 - mean_absolute_error: 1.0861 - mean_absolute_percentage_error: 29.3981\n",
      "Epoch 2/20\n",
      "176/176 - 2681s - loss: 0.5012 - mean_absolute_error: 0.5683 - mean_absolute_percentage_error: 15.9611\n",
      "Epoch 3/20\n",
      "176/176 - 2680s - loss: 0.3324 - mean_absolute_error: 0.4556 - mean_absolute_percentage_error: 12.6450\n",
      "Epoch 4/20\n",
      "176/176 - 2679s - loss: 0.2172 - mean_absolute_error: 0.3641 - mean_absolute_percentage_error: 10.0115\n",
      "Epoch 5/20\n",
      "176/176 - 2691s - loss: 0.1481 - mean_absolute_error: 0.3002 - mean_absolute_percentage_error: 8.2162\n",
      "Epoch 6/20\n",
      "176/176 - 2742s - loss: 0.0980 - mean_absolute_error: 0.2436 - mean_absolute_percentage_error: 6.6200\n",
      "Epoch 7/20\n",
      "176/176 - 2861s - loss: 0.0655 - mean_absolute_error: 0.1969 - mean_absolute_percentage_error: 5.3523\n",
      "Epoch 8/20\n",
      "176/176 - 2895s - loss: 0.0506 - mean_absolute_error: 0.1752 - mean_absolute_percentage_error: 4.7797\n",
      "Epoch 9/20\n",
      "176/176 - 2753s - loss: 0.0448 - mean_absolute_error: 0.1649 - mean_absolute_percentage_error: 4.5430\n",
      "Epoch 10/20\n",
      "176/176 - 2752s - loss: 0.0379 - mean_absolute_error: 0.1520 - mean_absolute_percentage_error: 4.1883\n",
      "Epoch 11/20\n",
      "176/176 - 2751s - loss: 0.0329 - mean_absolute_error: 0.1414 - mean_absolute_percentage_error: 3.8884\n",
      "Epoch 12/20\n",
      "176/176 - 2745s - loss: 0.0273 - mean_absolute_error: 0.1301 - mean_absolute_percentage_error: 3.5592\n",
      "Epoch 13/20\n",
      "176/176 - 2751s - loss: 0.0237 - mean_absolute_error: 0.1203 - mean_absolute_percentage_error: 3.3134\n",
      "Epoch 14/20\n",
      "176/176 - 2732s - loss: 0.0217 - mean_absolute_error: 0.1145 - mean_absolute_percentage_error: 3.1639\n",
      "Epoch 15/20\n",
      "176/176 - 2734s - loss: 0.0276 - mean_absolute_error: 0.1258 - mean_absolute_percentage_error: 3.6015\n",
      "44/44 [==============================] - 222s 5s/step - loss: 0.5297 - mean_absolute_error: 0.5822 - mean_absolute_percentage_error: 16.5811\n",
      "Completed 20 Epochs\n",
      "[0.529711127281189, 0.5822413563728333, 16.58112144470215]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E8 B32</th>\n",
       "      <td>0.503106</td>\n",
       "      <td>0.566886</td>\n",
       "      <td>16.075476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E20 B32</th>\n",
       "      <td>0.529711</td>\n",
       "      <td>0.582241</td>\n",
       "      <td>16.581121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1          2\n",
       "E8 B32   0.503106  0.566886  16.075476\n",
       "E20 B32  0.529711  0.582241  16.581121"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- NORMALISED TRAINING AND TESTING --\n",
    "normalised_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta=0.01)\n",
    "\n",
    "normalised_model.save_weights('normalised.h5')\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_model.load_weights(\"normalised.h5\")\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E20 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SAVE NORMALISED RESULTS --\n",
    "normalised_results = normalised_results.T\n",
    "normalised_results.to_csv(\"./lstm_side_information_tests/normalised_lexical.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --TEST WITH HUMAN GENERATED LABELS INCLUDED --\n",
    "#                                              #\n",
    "#                                              #\n",
    "#                                              #\n",
    "#                                              #\n",
    "#                                              #\n",
    "#                                              #\n",
    "#                                              #\n",
    "#                                              #\n",
    "# --TEST WITH HUMAN GENERATED LABELS INCLUDED --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv(\"../../../data/side_information.csv\",encoding='unicode_escape')\n",
    "checkpoint_path = \"C:/Users/jack-/Documents/University/Project/src/deep_learning/bert_tests/checkpoints\"\n",
    "feature_names = ['Sentence','Formality','Informativeness','Implicature','Length in Words','Length in Characters','F-score','I-score','Lexical Density','Adjective Density','Spellcheck Percentage','Syllable Ratio','Latinate vs Germanic']\n",
    "\n",
    "\n",
    "samples = data[feature_names]\n",
    "labels = data[\"Formality\"]\n",
    "train_samples, test_samples, train_labels,test_labels = train_test_split(samples, labels, test_size=0.2,random_state=5)\n",
    "\n",
    "bert_train_samples = np.array(train_samples[\"Sentence\"])\n",
    "bert_test_samples = np.array(test_samples[\"Sentence\"])\n",
    "side_train_samples = np.array(train_samples[feature_names[1:]])\n",
    "side_test_samples = np.array(test_samples[feature_names[1:]])\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "test_samples = np.array(test_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "\n",
    "# Attention layer\n",
    "class peel_the_layer(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self,units=1):    \n",
    "        ##Nothing special to be done here\n",
    "        super(peel_the_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        ##Define the shape of the weights and bias in this layer\n",
    "        ##This is a 1 unit layer. \n",
    "        units=1\n",
    "        ##last index of the input_shape is the number of dimensions of the prev\n",
    "        ##RNN layer. last but 1 index is the num of timesteps\n",
    "        self.w=self.add_weight(name=\"att_weights\", shape=(input_shape[-1], units), initializer=\"normal\") #name property is useful for avoiding RuntimeError: Unable to create link.\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[-2], units), initializer=\"zeros\")\n",
    "        super(peel_the_layer,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        ##x is the input tensor..each word that needs to be attended to\n",
    "        ##Below is the main processing done during training\n",
    "        ##K is the Keras Backend import\n",
    "        e = K.tanh(K.dot(x,self.w)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "\n",
    "        ##return the ouputs. 'a' is the set of attention weights\n",
    "        ##the second variable is the 'attention adjusted o/p state' or context\n",
    "        return a, K.sum(output, axis=1)\n",
    "\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "\n",
    "\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliser = tf.keras.layers.Normalization()\n",
    "normaliser.adapt(side_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NORMALISED SIDE INFORMATION MODEL --\n",
    "\n",
    "# Bert model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "normalised_bert = tf.keras.Model(text_input, lstm)\n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(12))\n",
    "normalised = normaliser(side_input)\n",
    "reshaped = tf.reshape(normalised,[-1, 1, 12])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "normalised_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Combine models and predict\n",
    "combined = tf.keras.layers.concatenate([normalised_bert.output, normalised_side.output],axis=1)\n",
    "a, context = peel_the_layer()(combined)\n",
    "dense = tf.keras.layers.Dense(1)(context)\n",
    "normalised_model = tf.keras.Model(inputs=[normalised_bert.input, normalised_side.input], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "176/176 - 2736s - loss: 3.1765 - mean_absolute_error: 1.2977 - mean_absolute_percentage_error: 36.1007\n",
      "Epoch 2/8\n",
      "176/176 - 2703s - loss: 1.0458 - mean_absolute_error: 0.8415 - mean_absolute_percentage_error: 25.0076\n",
      "Epoch 3/8\n",
      "176/176 - 2697s - loss: 0.9022 - mean_absolute_error: 0.7730 - mean_absolute_percentage_error: 22.8480\n",
      "Epoch 4/8\n",
      "176/176 - 2714s - loss: 0.8487 - mean_absolute_error: 0.7499 - mean_absolute_percentage_error: 21.7178\n",
      "Epoch 5/8\n",
      "176/176 - 2700s - loss: 0.7449 - mean_absolute_error: 0.6941 - mean_absolute_percentage_error: 20.1437\n",
      "Epoch 6/8\n",
      "176/176 - 2694s - loss: 0.7187 - mean_absolute_error: 0.6800 - mean_absolute_percentage_error: 19.9830\n",
      "Epoch 7/8\n",
      "176/176 - 2704s - loss: 0.9816 - mean_absolute_error: 0.8069 - mean_absolute_percentage_error: 23.9788\n",
      "Epoch 8/8\n",
      "176/176 - 2705s - loss: 1.0921 - mean_absolute_error: 0.8579 - mean_absolute_percentage_error: 25.7793\n",
      "44/44 [==============================] - 218s 5s/step - loss: 1.0731 - mean_absolute_error: 0.8359 - mean_absolute_percentage_error: 26.4065\n",
      "Completed 8 Epochs\n",
      "[1.073060154914856, 0.8358791470527649, 26.406518936157227]\n",
      "Epoch 1/20\n",
      "176/176 - 2714s - loss: 3.0148 - mean_absolute_error: 1.1875 - mean_absolute_percentage_error: 32.2335\n",
      "Epoch 2/20\n",
      "176/176 - 3197s - loss: 0.4789 - mean_absolute_error: 0.5570 - mean_absolute_percentage_error: 15.6394\n",
      "Epoch 3/20\n",
      "176/176 - 3411s - loss: 0.3063 - mean_absolute_error: 0.4371 - mean_absolute_percentage_error: 12.0677\n",
      "Epoch 4/20\n",
      "176/176 - 2908s - loss: 0.1767 - mean_absolute_error: 0.3282 - mean_absolute_percentage_error: 8.9360\n",
      "Epoch 5/20\n",
      "176/176 - 2740s - loss: 0.1186 - mean_absolute_error: 0.2673 - mean_absolute_percentage_error: 7.2899\n",
      "Epoch 6/20\n",
      "176/176 - 2760s - loss: 0.0716 - mean_absolute_error: 0.2098 - mean_absolute_percentage_error: 5.7324\n",
      "Epoch 7/20\n",
      "176/176 - 2786s - loss: 0.0760 - mean_absolute_error: 0.2042 - mean_absolute_percentage_error: 5.8567\n",
      "Epoch 8/20\n",
      "176/176 - 3114s - loss: 0.0672 - mean_absolute_error: 0.2014 - mean_absolute_percentage_error: 5.8513\n",
      "Epoch 9/20\n",
      "176/176 - 3060s - loss: 0.0461 - mean_absolute_error: 0.1675 - mean_absolute_percentage_error: 4.7343\n",
      "Epoch 10/20\n",
      "176/176 - 2925s - loss: 0.0346 - mean_absolute_error: 0.1445 - mean_absolute_percentage_error: 4.0647\n",
      "Epoch 11/20\n",
      "176/176 - 2931s - loss: 0.0244 - mean_absolute_error: 0.1201 - mean_absolute_percentage_error: 3.4137\n",
      "Epoch 12/20\n",
      "176/176 - 2864s - loss: 0.0222 - mean_absolute_error: 0.1159 - mean_absolute_percentage_error: 3.2312\n",
      "Epoch 13/20\n",
      "176/176 - 2831s - loss: 0.0226 - mean_absolute_error: 0.1180 - mean_absolute_percentage_error: 3.2549\n",
      "Epoch 14/20\n",
      "176/176 - 2798s - loss: 0.0226 - mean_absolute_error: 0.1167 - mean_absolute_percentage_error: 3.2395\n",
      "44/44 [==============================] - 225s 5s/step - loss: 0.4671 - mean_absolute_error: 0.5457 - mean_absolute_percentage_error: 15.5752\n",
      "Completed 20 Epochs\n",
      "[0.467096745967865, 0.5457094311714172, 15.575237274169922]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E8 B32</th>\n",
       "      <td>0.467097</td>\n",
       "      <td>0.545709</td>\n",
       "      <td>15.575237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1          2\n",
       "E8 B32  0.467097  0.545709  15.575237"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- NORMALISED TRAINING AND TESTING --\n",
    "normalised_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta=0.01)\n",
    "\n",
    "normalised_model.save_weights('normalised.h5')\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_model.load_weights(\"normalised.h5\")\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SAVE NORMALISED RESULTS --\n",
    "normalised_results = normalised_results.T\n",
    "normalised_results.to_csv(\"./lstm_side_information_tests/normalised_lexical_humans.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2292f7dadeec0b36dafabb7d1d8dd7b9b8b8f0665c515bec64e67bf9650aaf0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
